{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML_3031_TFRecord_Cats-Dogs.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1Nxxy_t_qNN0N_6WFFdKFuuuZgG93sajl","authorship_tag":"ABX9TyNi/0N3Zj8yD1e8MJeHPXHA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8bbc85eaa62f47608c236a3a08867d85":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_38f57c0c9a074582857728c4cbf1f627","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_08d314272a8149f990acd57405009180","IPY_MODEL_080d48384cb646ee83946a0f46881692"]}},"38f57c0c9a074582857728c4cbf1f627":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"08d314272a8149f990acd57405009180":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1cd8b3a8cb9b44eaaaf5afc618061c35","_dom_classes":[],"description":" 47%","_model_name":"FloatProgressModel","bar_style":"danger","max":800,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":375,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_717f3286c499490e8c1a5205e2594320"}},"080d48384cb646ee83946a0f46881692":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_25647ffaa1d44c4fbc97fb1274f43431","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 375/800 [03:39&lt;03:58,  1.78it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eae4986006594b00a970ead8fd60837d"}},"1cd8b3a8cb9b44eaaaf5afc618061c35":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"717f3286c499490e8c1a5205e2594320":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"25647ffaa1d44c4fbc97fb1274f43431":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"eae4986006594b00a970ead8fd60837d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FqHEmTLOEQwf","executionInfo":{"status":"ok","timestamp":1617157071301,"user_tz":-540,"elapsed":938,"user":{"displayName":"문성훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqiiwEhanmhSRxHyoC_siC-sixwe_kcfwG8CfkIA=s64","userId":"17340983977521415755"}},"outputId":"9d1d3bbc-88d7-499d-a4c8-388f4db87c0b"},"source":["list_a = ['a', 'b', 'c']\n","list_b = [1, 2, 3]\n","for x,y in zip(list_a,list_b):\n","    print(x,y)\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["a 1\n","b 2\n","c 3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":828,"referenced_widgets":["8bbc85eaa62f47608c236a3a08867d85","38f57c0c9a074582857728c4cbf1f627","08d314272a8149f990acd57405009180","080d48384cb646ee83946a0f46881692","1cd8b3a8cb9b44eaaaf5afc618061c35","717f3286c499490e8c1a5205e2594320","25647ffaa1d44c4fbc97fb1274f43431","eae4986006594b00a970ead8fd60837d"]},"id":"9ttrxxDBmaFy","executionInfo":{"status":"error","timestamp":1617163587333,"user_tz":-540,"elapsed":220040,"user":{"displayName":"문성훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqiiwEhanmhSRxHyoC_siC-sixwe_kcfwG8CfkIA=s64","userId":"17340983977521415755"}},"outputId":"e7b2b87a-f159-4459-e7ea-795ae51a8ff1"},"source":["# 개와 고양이 binary classification 예제의 TFRecord 버전\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tqdm.notebook import tqdm\n","from PIL import Image\n","import io\n","\n","############ Tensorflow에서 제공된 Type별 Feature 생성 코드 ############\n","\n","def _bytes_feature(value):\n","    # string / byte 타입을 받아서 byte list를 리턴.\n","    if isinstance(value, type(tf.constant(0))):\n","        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n","    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n","\n","def _float_feature(value):\n","    # float / double 타입을 받아서 float list를 리턴\n","    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n","\n","def _floatarray_feature(array):\n","    \"\"\"Returns a float_list from a float / double.\"\"\"\n","    return tf.train.Feature(float_list=tf.train.FloatList(value=array))\n","\n","def _int64_feature(value):\n","    # bool / enum / int / uint 타입을 받아서 int64 list를 리턴\n","    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n","\n","\n","def _validate_text(text):\n","    \"\"\"If text is not str or unicode, then try to convert it to str.\"\"\"\n","    if isinstance(text, str):\n","        return text\n","    elif isinstance(text, 'unicode'):\n","        return text.encode('utf8', 'ignore')\n","    else:\n","        return str(text)\n","\n","# DataFrame을 하나 생성해요!\n","# DataFrame의 column은 2개를 사용해요. (경로포함한 filename, label)\n","\n","src = '/content/drive/MyDrive/Machine Learning Colab/CAT_DOG/cat_dog_tfrecord/'\n","\n","df = pd.DataFrame(os.listdir(src),\n","                  columns=['filename'])\n","\n","df['label'] = ~df['filename'].str.contains('cat')\n","\n","df = df.astype({'label' : 'int'})\n","\n","df['filename'] = df['filename'].map(lambda x : src + x)\n","\n","display(df)\n","\n","### TFRecord 생성 함수 ###\n","\n","# id_list : 이미지 파일명을 가지고 있는 list\n","# label_list : 이미지 파일의 label을 가지고 있는 list\n","# tfrecords_name : tfrecord 파일의 이름(train, validation용도로 파일을 따로 만들어야 하기 때문)\n","\n","def to_tfrecords(id_list, label_list, tfrecords_name):\n","    print(\"converting 시작!!\")\n","\n","    options = tf.io.TFRecordOptions(compression_type='GZIP')\n","\n","    with tf.io.TFRecordWriter(path=os.path.join(tfrecords_name + '.tfrecords'),\n","                              options=options) as writer:\n","\n","        for id_, label_ in tqdm(zip(id_list,label_list),\n","                                total=len(id_list),\n","                                position=0,\n","                                leave=True):\n","            image_path = id_  # 이미지의 경로\n","            _binary_image = tf.io.read_file(image_path)  # 이미지 파일로부터 binary 데이터를 추출\n","\n","            my_features = tf.train.Features(feature={\n","                'image_raw': _bytes_feature(_binary_image),\n","                'label': _int64_feature(label_),\n","                'id': _bytes_feature(id_.encode())\n","            })\n","\n","            string_set = tf.train.Example(features=my_features)\n","\n","            writer.write(string_set.SerializeToString())\n","\n","train_ids, val_ids, train_label, val_label = \\\n","train_test_split(df['filename'],\n","                 df['label'],\n","                 test_size=0.2,\n","                 random_state=0)\n","\n","# to_tfrecords(train_ids,train_label,'cat_dog_train')\n","to_tfrecords(val_ids,val_label,'cat_dog_valid')"],"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/drive/MyDrive/Machine Learning Colab/...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/drive/MyDrive/Machine Learning Colab/...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/drive/MyDrive/Machine Learning Colab/...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/drive/MyDrive/Machine Learning Colab/...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/drive/MyDrive/Machine Learning Colab/...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3995</th>\n","      <td>/content/drive/MyDrive/Machine Learning Colab/...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3996</th>\n","      <td>/content/drive/MyDrive/Machine Learning Colab/...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3997</th>\n","      <td>/content/drive/MyDrive/Machine Learning Colab/...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3998</th>\n","      <td>/content/drive/MyDrive/Machine Learning Colab/...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3999</th>\n","      <td>/content/drive/MyDrive/Machine Learning Colab/...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4000 rows × 2 columns</p>\n","</div>"],"text/plain":["                                               filename  label\n","0     /content/drive/MyDrive/Machine Learning Colab/...      0\n","1     /content/drive/MyDrive/Machine Learning Colab/...      0\n","2     /content/drive/MyDrive/Machine Learning Colab/...      0\n","3     /content/drive/MyDrive/Machine Learning Colab/...      0\n","4     /content/drive/MyDrive/Machine Learning Colab/...      0\n","...                                                 ...    ...\n","3995  /content/drive/MyDrive/Machine Learning Colab/...      1\n","3996  /content/drive/MyDrive/Machine Learning Colab/...      1\n","3997  /content/drive/MyDrive/Machine Learning Colab/...      1\n","3998  /content/drive/MyDrive/Machine Learning Colab/...      1\n","3999  /content/drive/MyDrive/Machine Learning Colab/...      1\n","\n","[4000 rows x 2 columns]"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["converting 시작!!\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8bbc85eaa62f47608c236a3a08867d85","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=800.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-2f2379d7f0c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# to_tfrecords(train_ids,train_label,'cat_dog_train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mto_tfrecords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cat_dog_valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-2f2379d7f0c4>\u001b[0m in \u001b[0;36mto_tfrecords\u001b[0;34m(id_list, label_list, tfrecords_name)\u001b[0m\n\u001b[1;32m     75\u001b[0m                                 leave=True):\n\u001b[1;32m     76\u001b[0m             \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_\u001b[0m  \u001b[0;31m# 이미지의 경로\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0m_binary_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 이미지 파일로부터 binary 데이터를 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             my_features = tf.train.Features(feature={\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m       return read_file_eager_fallback(\n\u001b[0;32m--> 559\u001b[0;31m           filename, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m    560\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mread_file_eager_fallback\u001b[0;34m(filename, name, ctx)\u001b[0m\n\u001b[1;32m    595\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m   _result = _execute.execute(b\"ReadFile\", 1, inputs=_inputs_flat,\n\u001b[0;32m--> 597\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m    598\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     _execute.record_gradient(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"v3eNhaVudGge","executionInfo":{"status":"ok","timestamp":1617164255868,"user_tz":-540,"elapsed":1590,"user":{"displayName":"문성훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqiiwEhanmhSRxHyoC_siC-sixwe_kcfwG8CfkIA=s64","userId":"17340983977521415755"}},"outputId":"fdf1680b-3706-424e-8fe9-fddbac545ae3"},"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","# train, validation TFRecord 파일 경로\n","train_tfrecord_path = './cat_dog_train.tfrecords'\n","\n","\n","# TFRecord 파일을 불러와서 모델 학습 및 추론에 사용하기 위해서는 \n","# tf.data.Dataset 모듈과 유사한 기능을 가지는 tf.data.TFRecordDataset 모듈을 이용\n","# tf.data.Dataset 모듈은 여러 방식으로 데이터를 불러올 수 있는데 기본적으로는 \n","# generator로 부터 데이터를 불러온다. TFRecordDataset도 마찬가지임.\n","# 아래의 parameter는 TFRecordDataset를 사용할 때의 parameter\n","\n","BUFFER_SIZE = 256     # 데이터 shuffle을 위한 buffer size\n","BATCH_SIZE = 64       # 배치 사이즈. 한번에 가져오는 이미지 데이터 개수 \n","NUM_CLASS = 2         # class의 개수. binary인 경우는 필요없으며 categorical인 경우 설정\n","IMAGE_SIZE = 150       \n","\n","\n","# TFRecord를 읽어서 데이터를 복원하기 위한 자료구조.\n","image_feature_description = {\n","    'image_raw': tf.io.FixedLenFeature([], tf.string),\n","    'label': tf.io.FixedLenFeature([], tf.int64),\n","    'id': tf.io.FixedLenFeature([], tf.string),\n","}\n","\n","# 읽어들인 TFRecord를 다음의 형태(dict)로 변환하는 함수\n","# <ParallelMapDataset shapes: {id: (), image_raw: (), label: ()}, \n","#                     types: {id: tf.string, image_raw: tf.string, label: tf.int64}>\n","def _parse_image_function(example_proto):\n","    return tf.io.parse_single_example(example_proto, \n","                                      image_feature_description)\n","\n","# 위에서 얻은 ParallelMapDataset를 다음의 형태(shape)로 변환하는 함수\n","# <ParallelMapDataset shapes: ((None, None, 3), ()), types: (tf.float32, tf.int64)>\n","def map_func(target_record):\n","    img = target_record['image_raw']\n","    label = target_record['label']\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.dtypes.cast(img, tf.float32)\n","    return img, label\n","\n","\n","# 전처리(resize & augmentation) 함수\n","# 이미지 데이터 normalization\n","# 이 처리를 하는 대신 tf.keras.applications.mobilenet.preprocess_input() 함수를 이용하는것이 좋음\n","# 우리예제는 TFRecord 생성 시 원본 size로 저장했기 때문에 image resize를 해야함.\n","def image_resize_func(image, label):\n","    # result_image = image / 255\n","    result_image = tf.image.resize(image, (IMAGE_SIZE,IMAGE_SIZE))   \n","    return result_image, label\n","\n","\n","# 각각의 pretrained network마다 \n","# tensorflow.keras.applications.mobilenet.preprocess_input(image) 작업을 수행해야 함.\n","# 이부분에 대해서는 조금 더 알아봐야 함.\n","# 만약 multinomial classification이기 때문에 one_hot처리도 필요함.\n","# def post_process_func(image, label):\n","\n","# #     result_image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n","    # result_image = tf.keras.applications.mobilenet.preprocess_input(image)    \n","# #    onehot_label = tf.one_hot(label, depth=1049)    # binary인 경우 one_hot 사용안함.    \n","# #     return result_image, onehot_label\n","#     return result_image, label\n","\n","    \n","    \n","dataset = tf.data.TFRecordDataset(train_tfrecord_path, \n","                                  compression_type='GZIP')\n","dataset = dataset.map(_parse_image_function, \n","                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","dataset = dataset.map(map_func, \n","                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","dataset = dataset.cache()\n","# dataset shuffle 처리\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","\n","# 전처리(resize & auigmentation)\n","dataset = dataset.map(image_resize_func, \n","                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","# BatchDataset으로 변환\n","# <BatchDataset shapes: ((None, None, None, 3), (None,)), types: (tf.float32, tf.int64)>\n","# BatchDataset으로 변환하기 전에 image의 resize(전처리)가 일어나야 한다. 그렇지 않으면 \n","# shape이 달라 batch처리가 되지 않는다는 오류 발생.\n","dataset = dataset.batch(BATCH_SIZE)\n","\n","# pretrained network의 preprocess_input() 호출\n","# one_hot encoding 처리\n","# dataset = dataset.map(post_process_func, \n","#                       num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","# prefetch처리\n","# prefetch는 전처리와 학습과정의 모델 실행을 오버랩.\n","# 모델이 s스텝 학습을 실행하는 동안 입력 파이프라인은 s+1스텝의 데이터를 읽어서 수행속도를 높임.\n","# <PrefetchDataset shapes: ((None, None, None, 3), (None,)), types: (tf.float32, tf.int64)>\n","dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","\n","\n","# 아래의 코드는 데이터 확인을 위한 이미지 출력 코드.\n","# 1개의 batch만 얻어와서 그 중 첫번째 이미지만 출력\n","# 현재 batch size가 64이기 때문에 64개의 이미지를 가져온다.\n","# binary classification은 np.argmax()와 같은 처리가 필요없지만\n","# multinomial classification은 np.argmax()로 label 출력\n","\n","for batch_x, batch_y in dataset:\n","    print(batch_x.shape, batch_y.shape)\n","\n","    plt.figure()\n","    plt.imshow(batch_x[0].numpy())\n","    plt.title('label : {}'.format(np.argmax(batch_y[0])))\n","    plt.show()\n","\n","    break"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["(64, 150, 150, 3) (64,)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}